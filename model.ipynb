{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#export TF_MIN_GPU_MULTIPROCESSOR_COUNT=4\n",
    "#TF_MIN_GPU_MULTIPROCESSOR_COUNT=4\n",
    "#export CUDA_VISIBLE_DEVICES=0,1\n",
    "#CUDA_VISIBLE_DEVICES=0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "from contextlib import contextmanager\n",
    "import os\n",
    "import sys\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize(train_set):\n",
    "    mean = np.mean(train_set, axis=(0,1,2), keepdims=True)\n",
    "    std = np.std(train_set, axis=(0,1,2), keepdims=True)\n",
    "    return (train_set - mean) / std\n",
    "\n",
    "def scale_range(input, min=-1, max=1):\n",
    "\n",
    "    input += -(np.min(input))\n",
    "\n",
    "    input /= np.max(input) / (max - min)\n",
    "\n",
    "    input += min\n",
    "\n",
    "    return input\n",
    "\n",
    "def normalize(train_set, axis=(1,2)):\n",
    "    maxx = np.max(train_set, axis=axis, keepdims=True)\n",
    "    minn = np.min(train_set, axis=axis, keepdims=True)\n",
    "    return (train_set - minn) / (maxx - minn)\n",
    "    \n",
    "\n",
    "\n",
    "def load_image(addr):\n",
    "    # read an image and resize to (224, 224)\n",
    "    # cv2 load images as BGR, convert it to RGB\n",
    "    img = cv2.imread(addr)\n",
    "    img = cv2.resize(img, (320, 240))\n",
    "    img = normalize(img, axis=(0,1))\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def back_conv_layer(x, target_shape, kernel_size, stride_size, name, normalize = False, activation_function = False):\n",
    "        \n",
    "    with tf.variable_scope(name):\n",
    "        print(\"Back-Conv-Layer:\" + str(x.shape))\n",
    "        fan_in = int(x.shape[1] * x.shape[2])\n",
    "        depth = x.shape[-1]\n",
    "\n",
    "        if activation_function == tf.nn.relu or activation_function == tf.nn.leaky_relu:\n",
    "            var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "        else:\n",
    "            var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "        kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, target_shape[-1], depth], tf.float32, var_init)\n",
    "\n",
    "        var_init = tf.constant_initializer(0.0)\n",
    "        biases = tf.get_variable(\"biases\", [target_shape[-1]], initializer = var_init)\n",
    "\n",
    "        activation = tf.nn.conv2d_transpose(x, kernels, target_shape, strides = [1, stride_size, stride_size, 1]) + biases\n",
    "\n",
    "        if normalize:\n",
    "            activation = batch_norm(activation, [0, 1, 2])\n",
    "\n",
    "        return activation_function(activation) if callable(activation_function) else activation\n",
    "\n",
    "\n",
    "def _pop_batch_norm(x, pop_mean, pop_var, offset, scale):\n",
    "    return tf.nn.batch_normalization(x, pop_mean, pop_var, offset, scale, 1e-6)\n",
    "\n",
    "def _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale):\n",
    "    decay = 0.99\n",
    "    \n",
    "    dependency_1 = tf.assign(pop_mean, pop_mean * decay + mean * (1 - decay))\n",
    "    dependency_2 = tf.assign(pop_var, pop_var * decay + var * (1 - decay))\n",
    "\n",
    "    with tf.control_dependencies([dependency_1, dependency_2]):\n",
    "        return tf.nn.batch_normalization(x, mean, var, offset, scale, 1e-6)\n",
    "\n",
    "    \n",
    "\n",
    "def batch_norm(x, axes):\n",
    "    depth = x.shape[-1]\n",
    "    mean, var = tf.nn.moments(x, axes = axes)\n",
    "    \n",
    "    var_init = tf.constant_initializer(0.0)\n",
    "    offset = tf.get_variable(\"offset\", [depth], tf.float32, var_init)\n",
    "    var_init = tf.constant_initializer(1.0)\n",
    "    scale = tf.get_variable(\"scale\", [depth], tf.float32, var_init)\n",
    "    \n",
    "    pop_mean = tf.get_variable(\"pop_mean\", [depth], initializer = tf.zeros_initializer(), trainable = False)\n",
    "    pop_var = tf.get_variable(\"pop_var\", [depth], initializer = tf.ones_initializer(), trainable = False)\n",
    "    \n",
    "    return tf.cond(\n",
    "        is_training,\n",
    "        lambda: _batch_norm(x, pop_mean, pop_var, mean, var, offset, scale),\n",
    "        lambda: _pop_batch_norm(x, pop_mean, pop_var, offset, scale)\n",
    "    )\n",
    "\n",
    "def conv_layer(x, kernel_quantity, kernel_size, stride_size, name, normalize = False, activation_function = False):\n",
    "    #print(\"Conv-Layer:\" + str(x.shape))\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        depth = x.shape[-1]\n",
    "        fan_in = int(x.shape[1] * x.shape[2])\n",
    "\n",
    "        if activation_function == tf.nn.relu or activation_function == tf.nn.leaky_relu:\n",
    "            var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "        else:\n",
    "            var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "        kernels = tf.get_variable(\"kernels\", [kernel_size, kernel_size, depth, kernel_quantity], tf.float32, var_init)\n",
    "\n",
    "        var_init = tf.constant_initializer(0.0)\n",
    "        biases = tf.get_variable(\"biases\", [kernel_quantity], initializer = var_init)\n",
    "\n",
    "        activation = tf.nn.conv2d(x, kernels, strides = [1, stride_size, stride_size, 1], padding = \"SAME\") + biases\n",
    "        \n",
    "        if normalize:\n",
    "            activation = batch_norm(activation, [0, 1, 2])\n",
    "        \n",
    "        return activation_function(activation) if callable(activation_function) else activation\n",
    "    \n",
    "def flatten(x):\n",
    "    size = int(np.prod(x.shape[1:]))\n",
    "    return tf.reshape(x, [-1, size])\n",
    "\n",
    "\n",
    "def feed_forward_layer(x, target_size, name, normalize = False, activation_function = None):\n",
    "    with tf.variable_scope(name):\n",
    "    \n",
    "        fan_in = int(x.shape[-1])\n",
    "\n",
    "        if activation_function == tf.nn.relu or activation_function == tf.nn.leaky_relu:\n",
    "            var_init = tf.random_normal_initializer(stddev = 2/fan_in)\n",
    "        else:\n",
    "            var_init = tf.random_normal_initializer(stddev = fan_in**(-1/2))\n",
    "        weights = tf.get_variable(\"weights\", [x.shape[1], target_size], tf.float32, var_init)\n",
    "\n",
    "        var_init = tf.constant_initializer(0.0)\n",
    "        biases = tf.get_variable(\"biases\", [target_size], tf.float32, var_init)\n",
    "\n",
    "        activation = tf.matmul(x, weights) + biases\n",
    "\n",
    "        if normalize:\n",
    "            activation = batch_norm(activation, [0])\n",
    "\n",
    "        return activation_function(activation) if callable(activation_function) else activation\n",
    "\n",
    "    \n",
    "def batch_norm_2(x, is_training):\n",
    "    return tf.layers.batch_normalization(x, training=is_training)\n",
    "\n",
    "\n",
    "\n",
    "def norm(tensor):\n",
    "    return tf.div(\n",
    "        tf.subtract(tensor, tf.reduce_min(tensor)), \n",
    "        tf.subtract(tf.reduce_max(tensor), tf.reduce_min(tensor))\n",
    ")\n",
    "\n",
    "\n",
    "PS_OPS = ['Variable', 'VariableV2', 'AutoReloadVariable']\n",
    "\n",
    "def assign_to_device(device, ps_device='/cpu:0'):\n",
    "    def _assign(op):\n",
    "        node_def = op if isinstance(op, tf.NodeDef) else op.node_def\n",
    "        if node_def.op in PS_OPS:\n",
    "            return \"/\" + ps_device\n",
    "        else:\n",
    "            return device\n",
    "    return _assign \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def generator(images, image_size, reuse=False):\n",
    "        \n",
    "    with tf.variable_scope('gen', reuse=reuse):\n",
    "\n",
    "        half_sized_images = tf.image.resize_images(images, [image_size[0], image_size[1]])\n",
    "        base_model = tf.keras.applications.DenseNet169(input_shape=(None, None, 3),\n",
    "                                                       include_top=False, \n",
    "                                                       weights='imagenet')\n",
    "        pre_model = tf.keras.models.Model(inputs=base_model.input, outputs=base_model.get_layer('pool4_relu').output)\n",
    "        path_1 = pre_model(images)\n",
    "        path_2 = pre_model(half_sized_images)\n",
    "\n",
    "        #path_1 = tf.image.resize_bilinear(\n",
    "        #    path_1,[12, 20],\n",
    "        #)\n",
    "        path_2 = tf.image.resize_bilinear(\n",
    "            path_2,[15, 20],\n",
    "        )\n",
    "        encoder_output = tf.concat([path_1, path_2], 3)\n",
    "\n",
    "        decoder = conv_layer(encoder_output, 1024, 3, 1, \"conv_1_1\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 1024, 3, 1, \"conv_1_2\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 1024, 3, 1, \"conv_1_3\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = back_conv_layer(decoder, [BATCH_SIZE, 30, 40, 512], 3, 2, \"deconv_1\")\n",
    "\n",
    "        decoder = conv_layer(decoder, 512, 3, 1, \"conv_2_1\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 512, 3, 1, \"conv_2_2\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 512, 3, 1, \"conv_2_3\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = back_conv_layer(decoder, [BATCH_SIZE, 60, 80, 256], 3, 2, \"deconv_2\")\n",
    "\n",
    "        decoder = conv_layer(decoder, 256, 3, 1, \"conv_3_1\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 256, 3, 1, \"conv_3_2\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 256, 3, 1, \"conv_3_3\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = back_conv_layer(decoder, [BATCH_SIZE, 120, 160, 128], 3, 2, \"deconv_3\")\n",
    "\n",
    "        decoder = conv_layer(decoder, 128, 3, 1, \"conv_4_1\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 128, 3, 1, \"conv_4_2\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = back_conv_layer(decoder, [BATCH_SIZE, 240, 320, 64], 3, 2, \"deconv_4\")\n",
    "\n",
    "        decoder = conv_layer(decoder, 64, 3, 1, \"conv_5_1\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "        decoder = conv_layer(decoder, 64, 3, 1, \"conv_5_2\", normalize=True, activation_function = tf.nn.leaky_relu)\n",
    "\n",
    "        output = conv_layer(decoder, 1, 1, 1, \"conv1x1\", activation_function = tf.sigmoid)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator(x, reuse=False):\n",
    "\n",
    "    with tf.variable_scope('disc', reuse=reuse):\n",
    "\n",
    "        # Typical convolutional neural network to classify images.\n",
    "\n",
    "        x = tf.layers.conv2d(x, 64, 5)\n",
    "        \n",
    "\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = tf.layers.average_pooling2d(x, 2, 2)\n",
    "\n",
    "        x = tf.layers.conv2d(x, 128, 5)\n",
    "\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = tf.layers.average_pooling2d(x, 2, 2)\n",
    "        \n",
    "        x = tf.layers.conv2d(x, 256, 5)\n",
    "\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        x = tf.layers.average_pooling2d(x, 2, 2)\n",
    "\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "\n",
    "        # Output 2 classes: Real and Fake images\n",
    "\n",
    "        x = tf.layers.dense(x, 2)\n",
    "\n",
    "    return x\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval of filename lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset_files = np.array(os.listdir(\"./salicon_dataset/train/images/\"))\n",
    "trainmaps_files = np.array(os.listdir(\"./salicon_dataset/train/fixations/\"))\n",
    "\n",
    "\n",
    "valset_files = np.array(os.listdir(\"./salicon_dataset/val/images/\"))\n",
    "valmaps_files = np.array(os.listdir(\"./salicon_dataset/val/fixations/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for training\n",
    "\n",
    "\n",
    "if not os.path.exists('models/'):\n",
    "    os.makedirs('models/')\n",
    "if not os.path.exists('summaries/'):\n",
    "    os.makedirs('summaries/')\n",
    "    \n",
    "BATCH_SIZE = 5\n",
    "capacity = 10000\n",
    "threads = 1\n",
    "samples_nr = 10000\n",
    "epochs = 500\n",
    "pre_epochs = 25\n",
    "validation_step = 100\n",
    "num_gpus = 1\n",
    "steps_per_ep = samples_nr//(BATCH_SIZE*num_gpus)\n",
    "d_iter = 1\n",
    "\n",
    "#train_path = 'train.tfrecords'\n",
    "#val_path = 'val.tfrecords'\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "\n",
    "    g_content_losses = []\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    image_inputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE*num_gpus, 240, 320, 3))\n",
    "    image_size_placeholder = tf.placeholder(tf.int32, shape=(2))\n",
    "    is_training  = tf.placeholder(tf.bool)\n",
    "\n",
    "    #mode  = tf.placeholder(tf.string)\n",
    "    real_maps = tf.placeholder(tf.float32, shape=(BATCH_SIZE*num_gpus, 240, 320, 1))\n",
    "    \n",
    "    disc_target = tf.placeholder(tf.int32, shape=[BATCH_SIZE*2])\n",
    "    \n",
    "    g_global_step = tf.get_variable('g_global_step', trainable=False, initializer=0)\n",
    "\n",
    "    d_global_step = tf.get_variable('d_global_step', trainable=False, initializer=0)\n",
    "    \n",
    "    reuse_var = False\n",
    "    for i in range(num_gpus):\n",
    "\n",
    "        with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')):\n",
    "            \n",
    "            _image_inputs = image_inputs[i * BATCH_SIZE: (i+1) * BATCH_SIZE]\n",
    "\n",
    "            _real_maps = real_maps[i * BATCH_SIZE: (i+1) * BATCH_SIZE]\n",
    "            \n",
    "\n",
    "            gen_output = generator(_image_inputs, image_size_placeholder, reuse=reuse_var)\n",
    "\n",
    "            disc_real_input = tf.concat([_image_inputs, _real_maps], 3)\n",
    "            disc_fake_input = tf.concat([_image_inputs, gen_output], 3)\n",
    "\n",
    "\n",
    "            d_output_real = discriminator(disc_real_input, reuse=reuse_var)\n",
    "            d_output_fake = discriminator(disc_fake_input, reuse=True)\n",
    "\n",
    "            d_out_concat = tf.concat([d_output_real, d_output_fake], axis=0)\n",
    "            D_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=d_out_concat, \n",
    "                                                                                   labels=disc_target))\n",
    "\n",
    "            G_adv_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=d_output_fake, \n",
    "                                                                                   labels=disc_target[:BATCH_SIZE]))\n",
    "            \n",
    "\n",
    "            G_content_loss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(_real_maps, gen_output))\n",
    "\n",
    "            G_loss = G_adv_loss + (0.4*G_content_loss)\n",
    "\n",
    "            \n",
    "            \n",
    "            g_losses.append(G_loss)\n",
    "            d_losses.append(D_loss)\n",
    "            g_content_losses.append(G_content_loss)\n",
    "        \n",
    "        reuse_var = True\n",
    "  \n",
    "    \n",
    "    G_loss = tf.reduce_mean(tf.convert_to_tensor(g_losses))\n",
    "    G_content_loss = tf.reduce_mean(tf.convert_to_tensor(g_content_losses))\n",
    "    D_loss = tf.reduce_mean(tf.convert_to_tensor(d_losses))\n",
    "    \n",
    "    all_vars = tf.trainable_variables()\n",
    "    d_vars_dict = {}\n",
    "    g_vars_dict = {}\n",
    "    for var in all_vars:\n",
    "        if \"disc\" in var.name:\n",
    "            d_vars_dict[var.name] = var\n",
    "        if \"gen\" in var.name:\n",
    "            g_vars_dict[var.name] = var\n",
    "    d_vars = list(d_vars_dict.values())\n",
    "    g_vars = list(g_vars_dict.values())\n",
    "            \n",
    "    D_solver = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(D_loss, var_list=d_vars, \n",
    "                                                                     global_step=d_global_step,\n",
    "                                                                    colocate_gradients_with_ops=True)\n",
    "\n",
    "    G_pre_solver = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(G_content_loss, var_list=g_vars, \n",
    "                                                                         global_step=g_global_step,\n",
    "                                                                        colocate_gradients_with_ops=True)\n",
    "\n",
    "    G_solver = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(G_loss, var_list=g_vars, \n",
    "                                                                     global_step=g_global_step,\n",
    "                                                                    colocate_gradients_with_ops=True)\n",
    "    \n",
    "           \n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=5) \n",
    "\n",
    "\n",
    "    tf.summary.scalar('G_loss', G_loss)\n",
    "    tf.summary.scalar('G_content_loss', G_content_loss)\n",
    "    tf.summary.scalar('D_loss', D_loss)\n",
    "    #tf.summary.image(\"Images\", image_inputs[:3], max_outputs=3)\n",
    "    tf.summary.image(\"Generated\", gen_output[:3], max_outputs=3)\n",
    "    tf.summary.image(\"Original_maps\", real_maps[:3], max_outputs=3)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "\n",
    "    train_ids = np.array((range(len(trainset_files))))\n",
    "    val_ids = np.array((range(len(valset_files))))\n",
    "    \n",
    "    batch_disc_target = np.concatenate([np.ones([BATCH_SIZE]), np.zeros([BATCH_SIZE])], axis=0)\n",
    "\n",
    "    '''\n",
    "    ## create random batches when using tf_records\n",
    "    def get_batch(mode):\n",
    "        \n",
    "        feature = {'{}/image'.format(mode): tf.FixedLenFeature([], tf.string),\n",
    "                   '{}/fixation'.format(mode): tf.FixedLenFeature([], tf.string)}\n",
    "        # Create a list of filenames and pass it to a queue\n",
    "        filename_queue = tf.train.string_input_producer(['{}.tfrecords'.format(mode)], num_epochs=1)\n",
    "        # Define a reader and read the next record\n",
    "        reader = tf.TFRecordReader()\n",
    "        _, serialized_example = reader.read(filename_queue)\n",
    "        # Decode the record read by the reader\n",
    "        features = tf.parse_single_example(serialized_example, features=feature)\n",
    "        # Convert the image data from string back to the numbers\n",
    "        image = tf.decode_raw(features['{}/image'.format(mode)], tf.float32)\n",
    "\n",
    "        # Cast label data into int32\n",
    "        fixation = tf.decode_raw(features['{}/fixation'.format(mode)], tf.float32)\n",
    "        # Reshape image data into the original shape\n",
    "        image = tf.reshape(image, [240, 320, 3])\n",
    "        fixation = tf.reshape(fixation, [240, 320, 3])\n",
    "        # Any preprocessing here ...\n",
    "\n",
    "        # Creates batches by randomly shuffling tensors\n",
    "        imgs, fixations = tf.train.shuffle_batch([image, fixation], batch_size=BATCH_SIZE, capacity=15, num_threads=1, min_after_dequeue=10)\n",
    "\n",
    "        fixations = fixations[:,:,:,:1]\n",
    "        \n",
    "        return imgs, fixations\n",
    "    \n",
    "    tr_batch, tr_fixations = get_batch('train')\n",
    "    val_batch, val_fixations = get_batch('val')\n",
    "    '''\n",
    "    \n",
    "    config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    #init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('summaries/train', sess.graph)\n",
    "        val_writer = tf.summary.FileWriter('summaries/val', sess.graph)\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        #coord = tf.train.Coordinator()\n",
    "        #threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        sess.run(d_global_step.initializer)\n",
    "        sess.run(g_global_step.initializer)\n",
    "\n",
    "        for epoch in range(1, pre_epochs+1):\n",
    "\n",
    "            np.random.shuffle(train_ids)\n",
    "\n",
    "            for train_step in range(steps_per_ep):\n",
    "                \n",
    "                img_batch = trainset_files[train_ids[train_step*(BATCH_SIZE*num_gpus):train_step*(num_gpus*BATCH_SIZE)+(BATCH_SIZE*num_gpus)]]\n",
    "                map_batch = trainmaps_files[train_ids[train_step*(BATCH_SIZE*num_gpus):train_step*(BATCH_SIZE*num_gpus)+(BATCH_SIZE*num_gpus)]]\n",
    "                \n",
    "                images = np.array([cv2.resize(cv2.imread(\"./salicon_dataset/train/images/\" + file).astype(np.float32),\n",
    "                                              (320, 240))\n",
    "                                   for file in img_batch])\n",
    "                maps = np.array([np.expand_dims(cv2.resize(cv2.imread(\"./salicon_dataset/train/fixations/\" + file)[:,:,:1].astype(np.float32), \n",
    "                                           (320, 240)), axis=-1)\n",
    "                                     for file in map_batch])\n",
    "                \n",
    "                images = standarize(images)\n",
    "                maps = normalize(maps)\n",
    "                \n",
    "\n",
    "                #images, maps = sess.run([tr_batch, tr_fixations])\n",
    "                \n",
    "                \n",
    "                image_size = [images[0].shape[0]//2, images[0].shape[1]//2]\n",
    "\n",
    "\n",
    "                _ = sess.run(G_pre_solver, feed_dict={image_inputs:images, is_training:True, \n",
    "                                                      disc_target:batch_disc_target, image_size_placeholder:image_size,\n",
    "                                                    real_maps:maps},)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                if tf.train.global_step(sess, g_global_step) % validation_step == 0:\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={image_inputs:images, image_size_placeholder:image_size,\n",
    "                                                          disc_target:batch_disc_target,\n",
    "                                                                                real_maps:maps, is_training:False,},)\n",
    "\n",
    "\n",
    "                    train_writer.add_summary(summary, global_step=tf.train.global_step(sess, g_global_step))\n",
    "\n",
    "                    saver.save(sess, 'models/model.ckpt', global_step=tf.train.global_step(sess, g_global_step))\n",
    "\n",
    "                    \n",
    "                    np.random.shuffle(val_ids)\n",
    "                    \n",
    "                    img_batch = valset_files[val_ids[:(BATCH_SIZE*num_gpus)]]\n",
    "                    map_batch = valmaps_files[val_ids[:(BATCH_SIZE*num_gpus)]]\n",
    "\n",
    "                    \n",
    "                    images = np.array([cv2.resize(cv2.imread(\"./salicon_dataset/val/images/\" + file).astype(np.float32),\n",
    "                                              (320, 240))\n",
    "                                   for file in img_batch])\n",
    "                    maps = np.array([np.expand_dims(cv2.resize(cv2.imread(\"./salicon_dataset/val/fixations/\" + file)[:,:,:1].astype(np.float32), \n",
    "                                           (320, 240)), axis=-1)\n",
    "                                     for file in map_batch])\n",
    "\n",
    "                    images = standarize(images)\n",
    "                    maps = normalize(maps)\n",
    "                    \n",
    "                    \n",
    "                    #images, maps = sess.run([val_batch, val_fixations])\n",
    "\n",
    "                    image_size = [images[0].shape[0]//2, images[0].shape[1]//2]\n",
    "\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={image_inputs:images, image_size_placeholder:image_size,\n",
    "                                                          disc_target:batch_disc_target,\n",
    "                                                                                real_maps:maps, is_training:False,},)\n",
    "\n",
    "                    val_writer.add_summary(summary, global_step=tf.train.global_step(sess, g_global_step))\n",
    "\n",
    "                    print('Pre-Epoch: {}, step: {}'.format(epoch, tf.train.global_step(sess, g_global_step)))\n",
    "\n",
    "                    #print('D loss: {:.4}'. format(D_loss_curr))\n",
    "\n",
    "                    #print('G_loss: {:.4}'.format(gen_loss))\n",
    "\n",
    "                    #print('D_loss: {:.4}'.format(disc_loss))\n",
    "\n",
    "                    print()\n",
    "\n",
    "  \n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            np.random.shuffle(train_ids)\n",
    "\n",
    "            for train_step in range(steps_per_ep):\n",
    "                \n",
    "                \n",
    "                img_batch = trainset_files[train_ids[train_step*(BATCH_SIZE*num_gpus):train_step*(num_gpus*BATCH_SIZE)+(BATCH_SIZE*num_gpus)]]\n",
    "                map_batch = trainmaps_files[train_ids[train_step*(BATCH_SIZE*num_gpus):train_step*(BATCH_SIZE*num_gpus)+(BATCH_SIZE*num_gpus)]]\n",
    "                \n",
    "                images = np.array([cv2.resize(cv2.imread(\"./salicon_dataset/train/images/\" + file).astype(np.float32),\n",
    "                                              (320, 240))\n",
    "                                   for file in img_batch])\n",
    "                maps = np.array([np.expand_dims(cv2.resize(cv2.imread(\"./salicon_dataset/train/fixations/\" + file)[:,:,:1].astype(np.float32), \n",
    "                                           (320, 240)), axis=-1)\n",
    "                                     for file in map_batch])\n",
    "                \n",
    "                images = standarize(images)\n",
    "                maps = normalize(maps)\n",
    "                \n",
    "                \n",
    "                #images, maps = sess.run([tr_batch, tr_fixations])\n",
    "                image_size = [images[0].shape[0]//2, images[0].shape[1]//2]\n",
    "\n",
    "\n",
    "                _ = sess.run(G_solver, feed_dict={image_inputs:images, is_training:True,disc_target:batch_disc_target,\n",
    "                                                                                 image_size_placeholder:image_size,\n",
    "                                                                                real_maps:maps},)\n",
    "                \n",
    "                for _ in range(d_iter):\n",
    "                    _ = sess.run(D_solver, feed_dict={image_inputs:images, is_training:True,\n",
    "                                                      disc_target:batch_disc_target,image_size_placeholder:image_size,\n",
    "                                                                                real_maps:maps},)\n",
    "                    \n",
    "\n",
    "\n",
    "                if tf.train.global_step(sess, g_global_step) % validation_step == 0:\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={image_inputs:images, image_size_placeholder:image_size,\n",
    "                                                          disc_target:batch_disc_target,\n",
    "                                                                                real_maps:maps, is_training:False,},)\n",
    "\n",
    "                    train_writer.add_summary(summary, global_step=tf.train.global_step(sess, g_global_step))\n",
    "\n",
    "                    saver.save(sess, 'models/model.ckpt', global_step=tf.train.global_step(sess, g_global_step))\n",
    "                    \n",
    "                    np.random.shuffle(val_ids)\n",
    "                    \n",
    "                    img_batch = valset_files[val_ids[:(BATCH_SIZE*num_gpus)]]\n",
    "                    map_batch = valmaps_files[val_ids[:(BATCH_SIZE*num_gpus)]]\n",
    "\n",
    "                    \n",
    "                    images = np.array([cv2.resize(cv2.imread(\"./salicon_dataset/val/images/\" + file).astype(np.float32),\n",
    "                                              (320, 240))\n",
    "                                   for file in img_batch])\n",
    "                    maps = np.array([np.expand_dims(cv2.resize(cv2.imread(\"./salicon_dataset/val/fixations/\" + file)[:,:,:1].astype(np.float32), \n",
    "                                           (320, 240)), axis=-1)\n",
    "                                     for file in map_batch])\n",
    "\n",
    "                    images = standarize(images)\n",
    "                    maps = normalize(maps)\n",
    "                    \n",
    "                    \n",
    "                    #images, maps = sess.run([val_batch, val_fixations])\n",
    "                    image_size = [images[0].shape[0]//2, images[0].shape[1]//2]\n",
    "\n",
    "\n",
    "                    summary = sess.run(merged, feed_dict={image_inputs:images, image_size_placeholder:image_size,\n",
    "                                                          disc_target:batch_disc_target,\n",
    "                                                                                real_maps:maps, is_training:False,},)\n",
    "\n",
    "                    val_writer.add_summary(summary, global_step=tf.train.global_step(sess, g_global_step))\n",
    "\n",
    "                    print('Epoch: {}, step: {}'.format(epoch, tf.train.global_step(sess, g_global_step)))\n",
    "\n",
    "                    print()\n",
    "        \n",
    "\n",
    "        # Stop the threads\n",
    "        #coord.request_stop()\n",
    "\n",
    "        # Wait for threads to stop\n",
    "        #coord.join(threads)\n",
    "        #sess.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying results with test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for restoring models and inference\n",
    "BATCH_SIZE = 1\n",
    "capacity = 10000\n",
    "threads = 1\n",
    "\n",
    "validation_step = 50\n",
    "#ids = len(input_set) #//(BATCH_SIZE)\n",
    "\n",
    "\n",
    "image_inputs = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 240, 320, 3))\n",
    "image_size_placeholder = tf.placeholder(tf.int32, shape=(2))\n",
    "is_training  = tf.placeholder(tf.bool)\n",
    "\n",
    "real_maps = tf.placeholder(tf.float32, shape=(BATCH_SIZE, 240, 320, 1))\n",
    "\n",
    "g_global_step = tf.get_variable('g_global_step', trainable=False, initializer=0)\n",
    "\n",
    "\n",
    "\n",
    "gen_output = generator(image_inputs, image_size_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "all_vars = tf.global_variables()\n",
    "\n",
    "saver = tf.train.Saver(var_list=all_vars)\n",
    "\n",
    "\n",
    "config=tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "        saver.restore(sess,tf.train.latest_checkpoint('./models'))\n",
    "        \n",
    "\n",
    "        map_list = []\n",
    "        \n",
    "        for i in range(len(direc)):\n",
    "            image = cv2.imread(\"./test/images/\" + direc[i]).astype(np.float32)\n",
    "            \n",
    "            height, width = image.shape[0], image.shape[1]\n",
    "            image = np.expand_dims(cv2.resize(image, (320, 240)), axis=0)\n",
    "            image = standarize(image)\n",
    "\n",
    "            image_size = [image.shape[1]//2, image.shape[2]//2]\n",
    "\n",
    "\n",
    "            gen_map = sess.run(gen_output, feed_dict={image_inputs:image, is_training:False, \n",
    "                                                  image_size_placeholder:image_size,\n",
    "                                                },)\n",
    "            gen_map = cv2.resize(gen_map[0], (width, height))\n",
    "\n",
    "            map_list.append(gen_map.squeeze())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for showing results\n",
    "\n",
    "files = glob.glob(\"./test/images/*.jpg\")\n",
    "files_maps = glob.glob(\"./test/fixations/*.jpg\")\n",
    "files.sort()\n",
    "files_maps.sort()\n",
    "labels = [file[:-4] for file in files]\n",
    "map_labels = [file[:-4] for file in files_maps]\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "for i in range(len(files[:1])):\n",
    "    fig, ax = plt.subplots(3, 1)\n",
    "    img = np.array(Image.open(files[i]))\n",
    "    mapp = Image.open(files_maps[i])\n",
    "    ax[0].imshow(img)\n",
    "    ax[1].imshow(mapp, cmap=\"gray\")\n",
    "    ax[2].imshow(map_list[i], cmap=\"gray\")\n",
    "    ax[0].title.set_text(labels[i])\n",
    "    ax[1].title.set_text(map_labels[i])\n",
    "    ax[2].title.set_text(map_labels[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TF_records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer to http://machinelearninguru.com/deep_learning/tensorflow/basics/tfrecord/tfrecord.html\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "train_filename = 'train.tfrecords'  # address to save the TFRecords file\n",
    "# open the TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(train_filename)\n",
    "for i in range(len(valset_files)):\n",
    "    # print how many images are saved every 1000 images\n",
    "    if not i % 1000:\n",
    "        \n",
    "        print('train data: {}/{}'.format(i, len(valset_files)))\n",
    "        sys.stdout.flush()\n",
    "    # Load the image\n",
    "    img = load_image(\"./salicon_dataset/train/images/\"+valset_files[i])\n",
    "    mapp = load_image(\"./salicon_dataset/train/fixations/\"+valmaps_files[i])\n",
    "    #label = train_labels[i]\n",
    "    # Create a feature\n",
    "    feature = {'train/image': _bytes_feature(tf.compat.as_bytes(img.tostring())),\n",
    "               'train/fixation': _bytes_feature(tf.compat.as_bytes(mapp.tostring()))}\n",
    "    # Create an example protocol buffer\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "    # Serialize to string and write on the file\n",
    "    writer.write(example.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial results on Validation of SALICON dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8549757535615404\n"
     ]
    }
   ],
   "source": [
    "def cc(s_map,gt):\n",
    "\n",
    "    s_map_norm = (s_map - np.mean(s_map))/np.std(s_map)\n",
    "\n",
    "    gt_norm = (gt - np.mean(gt))/np.std(gt)\n",
    "\n",
    "    a = s_map_norm\n",
    "\n",
    "    b= gt_norm\n",
    "\n",
    "    r = (a*b).sum() / math.sqrt((a*a).sum() * (b*b).sum());\n",
    "\n",
    "    return r\n",
    "import math\n",
    "gt = list(os.listdir(\"./salicon_dataset/val/fixations/\"))\n",
    "gt.sort()\n",
    "pred = list(os.listdir(\"./salicon_val_pred/\"))\n",
    "pred.sort()\n",
    "cc_l = []\n",
    "for i in range(len(pred)):\n",
    "    imgg = cv2.imread(\"./salicon_dataset/val/fixations/\" + gt[i]).astype(np.float32)[:,:,0]\n",
    "    imgp = cv2.imread(\"./salicon_val_pred/\" + pred[i]).astype(np.float32)[:,:,0]\n",
    "    cc_l.append(cc(imgp,imgg))\n",
    "\n",
    "print(np.mean(cc_l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4015174\n"
     ]
    }
   ],
   "source": [
    "def kldiv(s_map,gt):\n",
    "\n",
    "    s_map = s_map/(np.sum(s_map)*1.0)\n",
    "\n",
    "    gt = gt/(np.sum(gt)*1.0)\n",
    "\n",
    "    eps = 2.2204e-16\n",
    "\n",
    "    return np.sum(gt * np.log(eps + gt/(s_map + eps)))\n",
    "\n",
    "import math\n",
    "gt = list(os.listdir(\"./salicon_dataset/val/fixations/\"))\n",
    "gt.sort()\n",
    "pred = list(os.listdir(\"./salicon_val_pred/\"))\n",
    "pred.sort()\n",
    "l = []\n",
    "for i in range(len(pred)):\n",
    "    imgg = cv2.imread(\"./salicon_dataset/val/fixations/\" + gt[i]).astype(np.float32)[:,:,0]\n",
    "    imgp = cv2.imread(\"./salicon_val_pred/\" + pred[i]).astype(np.float32)[:,:,0]\n",
    "    l.append(kldiv(imgp,imgg))\n",
    "\n",
    "print(np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
